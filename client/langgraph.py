"""
LangGraph Module with Centralized Pattern Configuration
Handles LangGraph agent creation, routing, and execution
"""
import asyncio
import json
import logging
import operator
import re
import time
from typing import TypedDict, Annotated, Sequence
from .stop_signal import is_stop_requested, clear_stop
from .langsearch_client import get_langsearch_client
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode

# Import only router patterns (for router() function)
from client.query_patterns import (
    ROUTER_INGEST_COMMAND, ROUTER_STATUS_QUERY, ROUTER_MULTI_STEP,
    ROUTER_ONE_TIME_INGEST, ROUTER_EXPLICIT_RAG, ROUTER_KNOWLEDGE_QUERY,
    ROUTER_EXCLUDE_MEDIA
)

# Try to import metrics
try:
    from metrics import metrics
    METRICS_AVAILABLE = True
except ImportError:
    try:
        from client.metrics import metrics
        METRICS_AVAILABLE = True
    except ImportError:
        METRICS_AVAILABLE = False
        from collections import defaultdict
        metrics = {
            "agent_runs": 0,
            "agent_errors": 0,
            "agent_times": [],
            "llm_calls": 0,
            "llm_errors": 0,
            "llm_times": [],
            "tool_calls": defaultdict(int),
            "tool_errors": defaultdict(int),
            "tool_times": defaultdict(list),
        }

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# CENTRALIZED PATTERN CONFIGURATION
# Add new intents here - no code changes needed!
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

INTENT_PATTERNS = {
    "general_knowledge": {
        "pattern": (
            r'\bwhat\s+is\s+(a|an|the)\s+\w+\??$'  # "what is a div?"
            r'|\bdefine\s+'
            r'|\bexplain\s+\w+\s*$'  # "explain recursion"
            r'|\bhow\s+does\s+\w+\s+work\??$'
        ),
        "tools": [],  # NO TOOLS - just answer from knowledge
        "priority": 4  # Lower priority than specific intents
    },
    "rag_status": {
        "pattern": (
            r'\bhow\s+many\s+.*(ingested|in\s+rag)\b'
            r'|\bwhat\s+(has|was)\s+been\s+ingested\b'
            r'|\bitems?\s+(have\s+been|were)\s+ingested\b'
            r'|\bcount\s+.*(items?|in\s+rag)\b'
            r'|\btotal\s+.*(items?|in\s+rag)\b'
            r'|\b(show|list|display)\s+rag\b'
            r'|\brag\s+(status|contents?|info|summary|overview|report)\b'
            r'|\bwhat(\'s| is)\s+in\s+(the\s+)?rag\b'
            r'|\bgive\s+me\s+rag\s+(stats|status|info|details)\b'
            r'|\bwhat\s+has\s+been\s+ingested\s+so\s+far\b'
            r'|\bwhat\s+did\s+you\s+ingest\b'
            r'|\bshow\s+me\s+everything\s+in\s+rag\b'
            r'|\brag\s+items\b'
            r'|\brag\s+dump\b'
            r'|\brag\s+data\b'
            r'|\bcurrent\s+rag\s+state\b'
        ),
        "tools": ["rag_status_tool", "rag_diagnose_tool"],
        "priority": 1
    },
    "ingest": {
        "pattern": (
            r'\bingest\b'
            r'|\bprocess\b'
            r'|\badd\s+to\s+rag\b'
            r'|\bindex\b'
            r'|\bvectorize\b'
            r'|\bembed\s+(this|it)\b'
            r'|\bupdate\s+rag\b'
            r'|\brefresh\s+rag\b'
            r'|\bscan\s+plex\b'
            r'|\bprocess\s+next\b'
            r'|\bingest\s+next\b'
            r'|\badd\s+movie\b'
            r'|\badd\s+media\b'
        ),
        "exclude_pattern": (
            r'\bhow\s+many\b'
            r'|\bwhat\s+(has|was)\b'
            r'|\bcount\b'
            r'|\btotal\b'
            r'|\bstatus\b'
            r'|\bwhat(\'s| is)\s+in\s+rag\b'
        ),
        "tools": [
            "plex_ingest_*",
            "plex_find_unprocessed",
            "plex_ingest_single",
            "plex_ingest_batch",
            "rag_add_tool"
        ],
        "priority": 2
    },
    "code_assistant": {
        "pattern": (
            # Analysis with "code" keyword
            r'\banalyze.*code\b'
            r'|\bcheck.*code\b'
            r'|\breview.*code\b'
            r'|\binspect.*code\b'
            r'|\blint\b'
            
            # Analysis with file extensions
            r'|\banalyze.*\.(py|js|jsx|ts|tsx|rs|go|java|kt)\b'
            r'|\bcheck.*\.(py|js|jsx|ts|tsx|rs|go|java|kt)\b'
            r'|\breview.*\.(py|js|jsx|ts|tsx|rs|go|java|kt)\b'
            r'|\binspect.*\.(py|js|jsx|ts|tsx|rs|go|java|kt)\b'
            
            # Fix requests
            r'|\bfix.*bug\b'
            r'|\bfix.*error\b'
            r'|\bfix.*issue\b'
            r'|\bfix.*code\b'
            r'|\bfix\s+this\b'
            r'|\bfix\s+my\b'
            r'|\bfix.*\.(py|js|jsx|ts|tsx|rs|go|java|kt)\b'
            
            # Quality/improvement
            r'|\bcode.*quality\b'
            r'|\bcode.*smell\b'
            r'|\bimprove.*code\b'
            r'|\brefactor\b'
            r'|\boptimize.*code\b'
            
            # Detection
            r'|\bfind.*bug\b'
            r'|\bdetect.*bug\b'
            r'|\banti.?pattern\b'
            r'|\bwhat\'?s?\s+wrong\s+with.*code\b'
            r'|\bissues?\s+in.*code\b'
            
            # Testing
            r'|\bgenerate.*test\b'
            r'|\bwrite.*test\b'
            r'|\bcreate.*test\b'
            
            # Direct file analysis
            r'|\banalyze.*my.*\.(py|js|jsx|ts|tsx|rs|go|java|kt)\b'
            r'|\bcheck.*my.*\.(py|js|jsx|ts|tsx|rs|go|java|kt)\b'
            
            # CODE GENERATION
            r'|\bgenerate.*code\b'
            r'|\bcreate.*(function|class|module|script|component)\b'
            r'|\bwrite.*(function|class|code)\b'
            r'|\bmake.*(function|class|component)\b'
            r'|\bbuild.*(function|class|component)\b'
            r'|\bcode\s+(for|that)\b'
            r'|\bfunction\s+that\b'
            r'|\bclass\s+that\b'
            r'|\bscript\s+(that|to)\b'
            r'|\bcomponent\s+that\b'
            
            # PROJECT ANALYSIS (NEW!)
            r'|\btech\s+stack\b'
            r'|\btechnology\s+stack\b'
            r'|\bwhat.*tech\b'
            r'|\bwhat.*technologies\b'
            r'|\bwhat.*languages\b'
            r'|\bwhat.*frameworks?\b'
            r'|\bwhat.*dependencies\b'
            r'|\bproject\s+structure\b'
            r'|\banalyze.*project\b'
            r'|\bscan.*project\b'
            r'|\blist.*dependencies\b'
            r'|\bshow.*structure\b'
            r'|\bdirectory\s+structure\b'
            r'|\bfolder\s+structure\b'
            r'|\bwhat\'?s?\s+in\s+requirements\b'
            r'|\bwhat\'?s?\s+in\s+package\.json\b'
        ),
        "tools": [
            "analyze_code_file",
            "fix_code_file",
            "suggest_improvements",
            "explain_code",
            "generate_tests",
            "refactor_code",
            "generate_code",
            "analyze_project",
            "get_project_dependencies",
            "scan_project_structure"
        ],
        "priority": 2
    },
    "location": {
        "pattern": (
            r'\b(my|what\'?s?\s+my)\s+location\b'
            r'|\bwhere\s+am\s+i\b'
            r'|\bcurrent\s+location\b'
            r'|\bwhere\s+do\s+i\s+live\b'
        ),
        "tools": ["get_location_tool"],
        "priority": 3
    },
    "weather": {
        "pattern": (
            r'\bweather\b'
            r'|\btemperature\b'
            r'|\bforecast\b'
            r'|\brain\b'
            r'|\bsnow\b'
            r'|\bwind\b'
            r'|\bconditions\b'
        ),
        "tools": ["get_location_tool", "get_weather_tool"],
        "priority": 3
    },
    "time": {
        "pattern": (
            r'\bwhat\s+time\b'
            r'|\bcurrent\s+time\b'
            r'|\btime\s+now\b'
            r'|\btime\s+is\s+it\b'
        ),
        "tools": ["get_time_tool"],
        "priority": 3
    },
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # CRITICAL FIX: plex_search BEFORE ml_recommendation (priority 2 vs 3)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    "plex_search": {
        "pattern": (
            # Direct search phrases
            r'\b(find|search|look\s+for|show\s+me)\s+.*\b(movie|film|show|media|series)\b'

            # Plot-based searches (CRITICAL for "where hero wins")
            r'|\bmovies?\s+(about|where|with|featuring|in\s+which)\b'
            r'|\bfilms?\s+(about|where|with|featuring|in\s+which)\b'

            # "where X happens" pattern
            r'|\bwhere\s+.*\s+(wins?|loses?|dies|survives|happens|occurs|escapes)\b'

            # Library references
            r'|\bsearch\s+(plex|library|my\s+library|my\s+movies)\b'
            r'|\bfind\s+.*\s+in\s+(plex|library|my\s+library)\b'

            # Scene searches
            r'|\bscene\s+(where|with|from)\b'
            r'|\bfind\s+scene\b'
            r'|\blocate\s+scene\b'

            # Browse/explore (not recommendations)
            r'|\bbrowse\s+my\b'
            r'|\blist\s+.*\s+(movies|films|shows)\b'
        ),
        "tools": [
            "rag_search_tool",
            "semantic_media_search_text",
            "scene_locator_tool",
            "find_scene_by_title"
        ],
        "priority": 2  # HIGHER priority than ml_recommendation!
    },
    "ml_recommendation": {
        "pattern": (
            # Explicit recommendation requests
            r'\brecommend(ation)?s?\b'
            r'|\bsuggest(ion)?s?\b'

            # ML/training specific
            r'|\bml\s+(model|train|recommendation)\b'
            r'|\btrain\s+(model|recommender|recommendation)\b'
            r'|\bauto.?train\b'

            # History management
            r'|\bimport\s+.*\s*history\b'
            r'|\bviewing\s+history\b'
            r'|\bwatch\s+history\b'
            r'|\brecord\s+(viewing|that\s+i\s+watched)\b'

            # Personalized suggestions
            r'|\bwhat\s+should\s+i\s+watch\b'
            r'|\brank\s+(these|movies|shows)\b'
            r'|\bmy\s+best\s+unwatched\b'
            r'|\bunwatched\s+(recommendations|suggestions)\b'

            # Stats/model info
            r'|\brecommender\s+stats\b'
        ),
        "tools": [
            "record_viewing",
            "train_recommender",
            "recommend_content",
            "get_recommender_stats",
            "import_plex_history",
            "auto_train_from_plex",
            "reset_recommender",
            "auto_recommend_from_plex"
        ],
        "priority": 3  # LOWER priority - only matches if plex_search doesn't
    },
    "system": {
        "pattern": (
            r'\bsystem\s+info\b'
            r'|\bhardware\b'
            r'|\b(cpu|gpu|ram)\b'
            r'|\bspecs?\b'
            r'|\bprocesses?\b'
            r'|\bperformance\b'
            r'|\butilization\b'
            r'|\bmemory\s+usage\b'
        ),
        "tools": [
            "get_hardware_specs_tool",
            "get_system_info",
            "list_system_processes",
            "terminate_process"
        ],
        "priority": 3
    },
    "code": {
        "pattern": (
            r'\bcode\b'
            r'|\bscan\s+code\b'
            r'|\bdebug\b'
            r'|\breview\s+code\b'
            r'|\bsummarize\s+code\b'
            r'|\bfix\s+this\s+code\b'
            r'|\bexplain\s+this\s+code\b'
        ),
        "tools": [
            "review_code",
            "summarize_code_file",
            "search_code_in_directory",
            "scan_code_directory",
            "summarize_code",
            "debug_fix"
        ],
        "priority": 3
    },
    "text": {
        "pattern": (
            r'\b(summarize|summary|explain|simplify|break\s+down)\b'
        ),
        "exclude_pattern": r'\bcode\b',
        "tools": [
            "summarize_text_tool",
            "summarize_direct_tool",
            "explain_simplified_tool",
            "concept_contextualizer_tool"
        ],
        "priority": 3
    },
    "todo": {
        "pattern": (
            r'\btodo\b'
            r'|\btask\b'
            r'|\bremind\s+me\b'
            r'|\bmy\s+todos?\b'
            r'|\bmy\s+tasks?\b'
            r'|\badd\s+to\s+my\s+list\b'
            r'|\btask\s+list\b'
        ),
        "tools": [
            "add_todo_item",
            "list_todo_items",
            "search_todo_items",
            "update_todo_item",
            "delete_todo_item",
            "delete_all_todo_items"
        ],
        "priority": 3
    },
    "knowledge": {
        "pattern": (
            r'\bremember\b'
            r'|\bsave\s+this\b'
            r'|\bmake\s+a\s+note\b'
            r'|\bknowledge\s+base\b'
            r'|\bsearch\s+my\s+notes?\b'
            r'|\badd\s+entry\b'
            r'|\bnote\s+this\b'
            r'|\bstore\s+this\b'
        ),
        "tools": [
            "add_entry",
            "list_entries",
            "get_entry",
            "search_entries",
            "search_by_tag",
            "search_semantic",
            "update_entry",
            "delete_entry"
        ],
        "priority": 3
    },
    "a2a": {
        "pattern": (
            r'\ba2a\b'
            r'|\bremote\s+(agent|tools?)\b'
            r'|\bdiscover\s+(agent|tools?)\b'
            r'|\bsend\s+to\s+remote\b'
            r'|\bcall\s+remote\s+tool\b'
            r'|\buse\s+remote\s+agent\b'
            r'|\bconnect\s+to\s+agent\b'
        ),
        "tools": ["send_a2a*", "discover_a2a"],
        "priority": 3
    }
}


class AgentState(TypedDict):
    """State that gets passed between nodes in the graph"""
    messages: Annotated[Sequence[BaseMessage], operator.add]
    tools: dict
    llm: object
    ingest_completed: bool
    stopped: bool


def router(state):
    """
    Route based on what the agent decided to do
    WITH STOP SIGNAL HANDLING AND A2A LOOP PREVENTION
    """
    last_message = state["messages"][-1]
    logger = logging.getLogger("mcp_client")
    logger.debug(f"üéØ Router: Last message type = {type(last_message).__name__}")

    # Stop signal check
    if is_stop_requested():
        logger.warning(f"üõë Router: Stop requested - ending graph execution")
        state["stopped"] = True
        return "continue"

    if state.get("stopped", False):
        logger.warning(f"üõë Router: Execution already stopped - ending")
        return "continue"

    # A2A completion check
    from langchain_core.messages import ToolMessage
    if isinstance(last_message, ToolMessage):
        if hasattr(last_message, 'name') and last_message.name in ["send_a2a", "discover_a2a",
                                                                   "send_a2a_streaming", "send_a2a_batch"]:
            logger.info(f"üõë Router: {last_message.name} result received - ending execution")
            return "continue"

    # If LLM made tool calls, execute them
    if isinstance(last_message, AIMessage):
        tool_calls = getattr(last_message, "tool_calls", [])
        if tool_calls and len(tool_calls) > 0:
            logger.debug(f"üéØ Router: Found {len(tool_calls)} tool calls - routing to TOOLS")
            return "tools"

    # Get user's original message
    user_message = None
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            user_message = msg
            break

    if user_message:
        content = user_message.content

        # Status query check
        if ROUTER_STATUS_QUERY.search(content):
            logger.info(f"üéØ Router: Status query detected - continuing normally")
            return "continue"

        # Ingest routing
        if ROUTER_INGEST_COMMAND.search(content) and not ROUTER_STATUS_QUERY.search(content):
            if not state.get("ingest_completed", False):
                if ROUTER_ONE_TIME_INGEST.search(content):
                    logger.info(f"üéØ Router: ONE-TIME ingest requested")
                    return "ingest"
                if ROUTER_MULTI_STEP.search(content):
                    logger.info(f"üéØ Router: INGEST with multiple steps")
                    return "continue"
                logger.info(f"üéØ Router: INGEST requested")
                return "ingest"
            else:
                logger.info(f"üéØ Router: Ingest already completed")
                return "continue"

        # Explicit RAG requests
        if ROUTER_EXPLICIT_RAG.search(content):
            logger.info(f"üéØ Router: Explicit RAG request")
            return "rag"

    # Default: continue to END
    logger.debug(f"üéØ Router: Continuing to END")
    return "continue"


async def rag_node(state):
    """Search RAG and provide context to answer the question"""
    logger = logging.getLogger("mcp_client")

    if is_stop_requested():
        logger.warning("üõë RAG node: Stop requested")
        msg = AIMessage(content="Search cancelled by user.")
        return {"messages": state["messages"] + [msg], "llm": state.get("llm"), "stopped": True}

    user_message = None
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            user_message = msg
            break

    if not user_message:
        logger.error("‚ùå No user message found in RAG node")
        msg = AIMessage(content="Error: Could not find user's question.")
        return {"messages": state["messages"] + [msg], "llm": state.get("llm")}

    # Find rag_search_tool
    tools_dict = state.get("tools", {})
    rag_search_tool = None
    for tool in tools_dict.values():
        if hasattr(tool, 'name') and tool.name == "rag_search_tool":
            rag_search_tool = tool
            break

    if not rag_search_tool:
        logger.error(f"‚ùå RAG search tool not found")
        msg = AIMessage(content="RAG search is not available.")
        return {"messages": state["messages"] + [msg], "llm": state.get("llm")}

    try:
        # Call RAG search
        result = await rag_search_tool.ainvoke({"query": user_message.content})

        # Parse results (simplified - add your parsing logic here)
        context = "RAG search results here"

        # Create augmented message with RAG context
        augmented_messages = [
            SystemMessage(content=f"Context from RAG:\n\n{context}"),
            user_message
        ]

        llm = state.get("llm")
        response = await llm.ainvoke(augmented_messages)

        return {"messages": state["messages"] + [response], "llm": state.get("llm")}

    except Exception as e:
        logger.error(f"‚ùå Error in RAG node: {e}")
        msg = AIMessage(content=f"Error searching knowledge base: {str(e)}")
        return {"messages": state["messages"] + [msg], "llm": state.get("llm")}


def create_langgraph_agent(llm_with_tools, tools):
    """Create and compile the LangGraph agent"""
    logger = logging.getLogger("mcp_client")

    base_llm = llm_with_tools.bound if hasattr(llm_with_tools, 'bound') else llm_with_tools

    async def call_model(state: AgentState):
        if is_stop_requested():
            logger.warning("üõë call_model: Stop requested")
            empty_response = AIMessage(content="Operation cancelled by user.")
            return {
                "messages": state["messages"] + [empty_response],
                "tools": state.get("tools", {}),
                "llm": state.get("llm"),
                "ingest_completed": state.get("ingest_completed", False),
                "stopped": True
            }

        messages = state["messages"]
        from langchain_core.messages import ToolMessage

        last_message = messages[-1] if messages else None

        # If formatting tool results, use base LLM
        if isinstance(last_message, ToolMessage):
            logger.info("üéØ Formatting tool results")
            start_time = time.time()
            try:
                response = await asyncio.wait_for(
                    base_llm.ainvoke(messages),
                    timeout=300.0
                )
                duration = time.time() - start_time
                if METRICS_AVAILABLE:
                    metrics["llm_calls"] += 1
                    metrics["llm_times"].append((time.time(), duration))
                return {
                    "messages": messages + [response],
                    "tools": state.get("tools", {}),
                    "llm": state.get("llm"),
                    "ingest_completed": state.get("ingest_completed", False),
                    "stopped": state.get("stopped", False)
                }
            except asyncio.TimeoutError:
                duration = time.time() - start_time
                if METRICS_AVAILABLE:
                    metrics["llm_errors"] += 1
                    metrics["llm_times"].append((time.time(), duration))
                logger.error(f"‚è±Ô∏è LLM call timed out after 5m")

                # Return helpful timeout message instead of crashing
                timeout_message = AIMessage(content="""‚è±Ô∏è Request timed out after 5 minutes.

                **The model is taking too long to respond.** This usually happens when:
                - The model is processing too many tools (58 tools detected)
                - The query is ambiguous and the model is stuck deciding
                - The model is overloaded

                **Try these solutions:**
                1. Rephrase your question more specifically
                2. Break complex questions into smaller parts
                3. Restart the Ollama service: `ollama restart`

                **Your question:** {question}""".format(question=messages[-1].content if messages else "unknown"))

                return {
                    "messages": messages + [timeout_message],
                    "tools": state.get("tools", {}),
                    "llm": state.get("llm"),
                    "ingest_completed": state.get("ingest_completed", False),
                    "stopped": state.get("stopped", False)
                }

            except Exception as e:
                duration = time.time() - start_time
                if METRICS_AVAILABLE:
                    metrics["llm_errors"] += 1
                    metrics["llm_times"].append((time.time(), duration))
                logger.error(f"‚ùå Model call failed: {e}")
                raise

        # Get user message
        user_message = None
        for msg in reversed(messages):
            if isinstance(msg, HumanMessage):
                user_message = msg.content
                break

        # Force LangSearch if explicitly requested
        if user_message:
            user_lower = user_message.lower()

            # Check for explicit langsearch requests
            langsearch_patterns = [
                r'\buse\s+langsearch\b',
                r'\busing\s+langsearch\b',
                r'\bwith\s+langsearch\b',
                r'\blangsearch\s+for\b',
                r'\bvia\s+langsearch\b',
            ]

            should_use_langsearch = any(
                re.search(pattern, user_lower)
                for pattern in langsearch_patterns
            )

            if should_use_langsearch:
                logger.info("üéØ FORCED LANGSEARCH: User explicitly requested langsearch")

                langsearch = get_langsearch_client()

                if langsearch.is_available():
                    # Extract the actual query (remove routing keywords)
                    query = user_message
                    for pattern in langsearch_patterns:
                        query = re.sub(pattern, '', query, flags=re.IGNORECASE)
                    query = query.strip()

                    # Remove common leading words
                    query = re.sub(r'^,?\s*(who|what|where|when|why|how)\s+', r'\1 ', query, flags=re.IGNORECASE)
                    query = query.strip()

                    if not query:
                        query = user_message

                    logger.info(f"üîç Searching with langsearch: '{query}'")

                    try:
                        search_result = await langsearch.search(query)

                        if search_result["success"] and search_result["results"]:
                            logger.info("‚úÖ LangSearch successful - passing to LLM for processing")
                            search_context = search_result["results"]

                            # Create augmented prompt for LLM to process
                            augmented_prompt = f"""I searched the web using LangSearch and found the following results:

        {search_context}

        Based on these search results, please answer the user's question: "{user_message}"

        Provide a clear, concise answer in English. Extract the most relevant information and present it naturally."""

                            # Let LLM process the search results
                            augmented_messages = messages + [HumanMessage(content=augmented_prompt)]
                            response = await base_llm.ainvoke(augmented_messages)

                            return {
                                "messages": messages + [response],
                                "tools": state.get("tools", {}),
                                "llm": state.get("llm"),
                                "ingest_completed": state.get("ingest_completed", False),
                                "stopped": state.get("stopped", False)
                            }
                        else:
                            logger.warning("‚ö†Ô∏è LangSearch returned no results")

                    except Exception as e:
                        logger.error(f"‚ùå LangSearch failed: {e}")

                else:
                    logger.warning("‚ö†Ô∏è LangSearch not available")
                    error_response = AIMessage(
                        content="LangSearch is not available. Please check configuration."
                    )
                    return {
                        "messages": messages + [error_response],
                        "tools": state.get("tools", {}),
                        "llm": state.get("llm"),
                        "ingest_completed": state.get("ingest_completed", False),
                        "stopped": state.get("stopped", False)
                    }

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # CENTRALIZED PATTERN MATCHING
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        def match_intent(user_message: str, all_tools: list, base_llm, logger):
            """
            Match user intent using centralized pattern configuration.
            Tools are filtered automatically based on INTENT_PATTERNS.
            """
            # Sort patterns by priority (lower number = higher priority)
            sorted_patterns = sorted(INTENT_PATTERNS.items(), key=lambda x: x[1]["priority"])

            for intent_name, config in sorted_patterns:
                # Check if pattern matches
                if re.search(config["pattern"], user_message, re.IGNORECASE):
                    # Check exclude pattern if present
                    if "exclude_pattern" in config:
                        if re.search(config["exclude_pattern"], user_message, re.IGNORECASE):
                            continue  # Skip this intent

                    logger.info(f"üéØ {intent_name} ‚Üí filtering tools")

                    # Filter tools based on tool patterns
                    filtered_tools = []
                    for tool in all_tools:
                        for tool_pattern in config["tools"]:
                            # Handle wildcard matching (e.g., "plex_ingest_*")
                            if "*" in tool_pattern:
                                prefix = tool_pattern.replace("*", "")
                                if tool.name.startswith(prefix):
                                    filtered_tools.append(tool)
                                    break
                            # Exact match
                            elif tool.name == tool_pattern:
                                filtered_tools.append(tool)
                                break

                    if filtered_tools:
                        logger.info(f"   ‚Üí {len(filtered_tools)} tools: {[t.name for t in filtered_tools[:5]]}")
                        return base_llm.bind_tools(filtered_tools), intent_name

            # No pattern matched - give all tools
            logger.info(f"üéØ General query ‚Üí all {len(all_tools)} tools")
            return base_llm.bind_tools(all_tools), "general"

        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        # APPLY PATTERN MATCHING
        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        if user_message:
            all_tools = list(state.get("tools", {}).values())
            llm_to_use, pattern_name = match_intent(user_message, all_tools, base_llm, logger)
        else:
            llm_to_use = llm_with_tools

        logger.info(f"üß† Calling LLM with {len(messages)} messages")

        start_time = time.time()
        try:
            # Add 5 minutes timeout to prevent hanging
            response = await asyncio.wait_for(
                llm_to_use.ainvoke(messages),
                timeout=300.0
            )
            duration = time.time() - start_time

            if METRICS_AVAILABLE:
                metrics["llm_calls"] += 1
                metrics["llm_times"].append((time.time(), duration))

            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # FALLBACK CHAIN: Tools ‚Üí LangSearch ‚Üí Base LLM
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            has_tool_calls = hasattr(response, 'tool_calls') and response.tool_calls
            has_content = hasattr(response, 'content') and response.content and response.content.strip()

            if not has_tool_calls and not has_content:
                # Completely blank - try LangSearch
                logger.warning("‚ö†Ô∏è LLM returned blank - trying LangSearch")
                langsearch = get_langsearch_client()

                if langsearch.is_available():
                    search_result = await langsearch.search(user_message)

                    if search_result["success"] and search_result["results"]:
                        logger.info("‚úÖ LangSearch successful")
                        search_context = search_result["results"]
                        augmented_prompt = f"""WEB SEARCH RESULTS:
{search_context}

Please answer the question using these search results."""

                        retry_messages = messages + [HumanMessage(content=augmented_prompt)]
                        response = await base_llm.ainvoke(retry_messages)
                    else:
                        # LangSearch failed - use base LLM
                        logger.warning("‚ö†Ô∏è LangSearch failed - using base LLM")
                        response = await asyncio.wait_for(
                            base_llm.ainvoke(messages),
                            timeout=300.0
                        )
                else:
                    logger.warning("‚ö†Ô∏è LangSearch unavailable - using base LLM")
                    response = await asyncio.wait_for(
                        base_llm.ainvoke(messages),
                        timeout=300.0
                    )

            elif not has_tool_calls and has_content:
                # Has content but no tools - check if needs current info
                needs_current_info = any(word in user_message.lower() for word in [
                    "current", "who is", "latest", "recent", "today", "now"
                ])

                if needs_current_info:
                    logger.info("üîç Trying LangSearch fallback for current info")
                    langsearch = get_langsearch_client()

                    if langsearch.is_available():
                        search_result = await langsearch.search(user_message)

                        if search_result["success"] and search_result["results"]:
                            logger.info("‚úÖ LangSearch successful - augmenting")
                            search_context = search_result["results"]
                            augmented_prompt = f"""Previous answer: {response.content}

However, here are current web search results:
{search_context}

Please provide an updated answer using these search results."""

                            retry_messages = messages + [response, HumanMessage(content=augmented_prompt)]
                            response = await base_llm.ainvoke(retry_messages)

            return {
                "messages": messages + [response],
                "tools": state.get("tools", {}),
                "llm": state.get("llm"),
                "ingest_completed": state.get("ingest_completed", False),
                "stopped": state.get("stopped", False)
            }

        except asyncio.TimeoutError:
            duration = time.time() - start_time
            if METRICS_AVAILABLE:
                metrics["llm_errors"] += 1
                metrics["llm_times"].append((time.time(), duration))
            logger.error(f"‚è±Ô∏è LLM call timed out after 5m")

            # Return helpful timeout message instead of crashing
            return {
                "messages": messages + [AIMessage(
                    content="‚è±Ô∏è Request timed out after 5 minutes. Please try:\n\n1. Rephrasing your question\n2. Breaking it into smaller parts\n3. Using a simpler query")],
                "tools": state.get("tools", {}),
                "llm": state.get("llm"),
                "ingest_completed": state.get("ingest_completed", False),
                "stopped": state.get("stopped", False)
            }

        except Exception as e:
            duration = time.time() - start_time
            if METRICS_AVAILABLE:
                metrics["llm_errors"] += 1
                metrics["llm_times"].append((time.time(), duration))
            logger.error(f"‚ùå Model call failed: {e}")
            raise

    async def ingest_node(state: AgentState):
        """Handle ingestion operations"""
        if is_stop_requested():
            logger.warning("üõë ingest_node: Stop requested")
            msg = AIMessage(content="Ingestion cancelled by user.")
            return {
                "messages": state["messages"] + [msg],
                "tools": state.get("tools", {}),
                "llm": state.get("llm"),
                "ingest_completed": True,
                "stopped": True
            }

        tools_dict = state.get("tools", {})
        ingest_tool = None
        for tool in tools_dict.values():
            if hasattr(tool, 'name') and tool.name == "plex_ingest_batch":
                ingest_tool = tool
                break

        if not ingest_tool:
            msg = AIMessage(content="Ingestion tool not available.")
            return {
                "messages": state["messages"] + [msg],
                "tools": state.get("tools", {}),
                "llm": state.get("llm"),
                "ingest_completed": True,
                "stopped": False
            }

        try:
            logger.info("üì• Starting ingest operation...")
            result = await ingest_tool.ainvoke({"limit": 5})
            msg = AIMessage(content=f"Ingestion complete: {result}")
            return {
                "messages": state["messages"] + [msg],
                "tools": state.get("tools", {}),
                "llm": state.get("llm"),
                "ingest_completed": True,
                "stopped": False
            }
        except Exception as e:
            logger.error(f"‚ùå Error in ingest_node: {e}")
            msg = AIMessage(content=f"Ingestion failed: {str(e)}")
            return {
                "messages": state["messages"] + [msg],
                "tools": state.get("tools", {}),
                "llm": state.get("llm"),
                "ingest_completed": True,
                "stopped": False
            }

    async def call_tools_with_stop_check(state: AgentState):
        """Execute tools with stop signal checking"""
        logger = logging.getLogger("mcp_client")

        if is_stop_requested():
            logger.warning("üõë call_tools: Stop requested")
            empty_response = AIMessage(content="Tool execution cancelled by user.")
            return {
                "messages": state["messages"] + [empty_response],
                "tools": state.get("tools", {}),
                "llm": state.get("llm"),
                "ingest_completed": state.get("ingest_completed", False),
                "stopped": True
            }

        from langchain_core.messages import ToolMessage
        last_message = state["messages"][-1]
        tool_calls = getattr(last_message, "tool_calls", [])

        if not tool_calls:
            logger.warning("‚ö†Ô∏è No tool calls found")
            return state

        tool_messages = []
        for tool_call in tool_calls:
            if is_stop_requested():
                logger.warning(f"üõë Stop requested - halting tool calls")
                break

            tool_name = tool_call.get("name")
            tool_args = tool_call.get("args", {})
            tool_id = tool_call.get("id")

            logger.info(f"üîß Executing tool: {tool_name}")

            tools_dict = state.get("tools", {})
            tool = tools_dict.get(tool_name)

            if not tool:
                logger.error(f"‚ùå Tool '{tool_name}' not found")
                error_msg = ToolMessage(
                    content=f"Error: Tool '{tool_name}' not found",
                    tool_call_id=tool_id,
                    name=tool_name
                )
                tool_messages.append(error_msg)
                continue

            try:
                tool_start = time.time()
                result = await tool.ainvoke(tool_args)
                tool_duration = time.time() - tool_start

                if METRICS_AVAILABLE:
                    metrics["tool_calls"][tool_name] += 1
                    metrics["tool_times"][tool_name].append((time.time(), tool_duration))

                if isinstance(result, list) and len(result) > 0:
                    if hasattr(result[0], 'text'):
                        result = result[0].text

                result_msg = ToolMessage(
                    content=str(result),
                    tool_call_id=tool_id,
                    name=tool_name
                )
                tool_messages.append(result_msg)
                logger.info(f"‚úÖ Tool {tool_name} completed in {tool_duration:.2f}s")

            except Exception as e:
                logger.error(f"‚ùå Tool {tool_name} failed: {e}")
                if METRICS_AVAILABLE:
                    metrics["tool_errors"][tool_name] += 1
                error_msg = ToolMessage(
                    content=f"Error: {str(e)}",
                    tool_call_id=tool_id,
                    name=tool_name
                )
                tool_messages.append(error_msg)

        return {
            "messages": state["messages"] + tool_messages,
            "tools": state.get("tools", {}),
            "llm": state.get("llm"),
            "ingest_completed": state.get("ingest_completed", False),
            "stopped": is_stop_requested()
        }

    # Build graph
    workflow = StateGraph(AgentState)
    workflow.add_node("agent", call_model)
    workflow.add_node("tools", call_tools_with_stop_check)
    workflow.add_node("rag", rag_node)
    workflow.add_node("ingest", ingest_node)

    workflow.set_entry_point("agent")
    workflow.add_conditional_edges(
        "agent",
        router,
        {
            "tools": "tools",
            "rag": "rag",
            "ingest": "ingest",
            "continue": END
        }
    )
    workflow.add_edge("tools", "agent")
    workflow.add_edge("ingest", END)
    workflow.add_edge("rag", END)

    app = workflow.compile()
    logger.info("‚úÖ LangGraph agent compiled successfully")
    return app


async def run_agent(agent, conversation_state, user_message, logger, tools, system_prompt, llm=None, max_history=20):
    """Execute the agent with the given user message and track metrics"""
    start_time = time.time()
    clear_stop()
    logger.info("‚úÖ Stop signal cleared for new request")

    try:
        if METRICS_AVAILABLE:
            metrics["agent_runs"] += 1

        if not conversation_state["messages"]:
            conversation_state["messages"].append(SystemMessage(content=system_prompt))

        conversation_state["messages"].append(HumanMessage(content=user_message))
        conversation_state["messages"] = conversation_state["messages"][-max_history:]

        if not isinstance(conversation_state["messages"][0], SystemMessage):
            conversation_state["messages"].insert(0, SystemMessage(content=system_prompt))

        logger.info(f"üß† Starting agent with {len(conversation_state['messages'])} messages")

        tool_registry = {tool.name: tool for tool in tools}

        result = await agent.ainvoke({
            "messages": conversation_state["messages"],
            "tools": tool_registry,
            "llm": llm,
            "ingest_completed": False,
            "stopped": False
        })

        new_messages = result["messages"][len(conversation_state["messages"]):]
        logger.info(f"üì® Agent added {len(new_messages)} new messages")
        conversation_state["messages"].extend(new_messages)

        if METRICS_AVAILABLE:
            duration = time.time() - start_time
            metrics["agent_times"].append((time.time(), duration))
            logger.info(f"‚úÖ Agent run completed in {duration:.2f}s")

        return {"messages": conversation_state["messages"]}

    except ValueError as e:
        # Handle context window overflow gracefully
        error_str = str(e)
        if "exceed context window" in error_str or "Requested tokens" in error_str:
            # Extract token counts from error message
            import re
            match = re.search(r'Requested tokens \((\d+)\) exceed context window of (\d+)', error_str)

            if match:
                requested = int(match.group(1))
                available = int(match.group(2))
                overflow = requested - available
                logger.error(
                    f"‚ùå Context overflow: {requested} tokens requested, {available} available ({overflow} over)")
            else:
                requested = None
                available = None
                logger.error(f"‚ùå Context window overflow")

            # Calculate how many messages to drop
            current_msg_count = len(conversation_state["messages"])
            if current_msg_count > 3:  # Keep at least system + user message
                # Drop half the history
                new_limit = max(3, current_msg_count // 2)
                logger.warning(f"‚ö†Ô∏è  Auto-recovery: Reducing history from {current_msg_count} to {new_limit} messages")

                # Keep system message + trim history + keep latest user message
                system_msg = conversation_state["messages"][0] if isinstance(conversation_state["messages"][0],
                                                                             SystemMessage) else None
                user_msg = conversation_state["messages"][-1]
                middle_msgs = conversation_state["messages"][1:-1]

                # Take most recent middle messages
                trimmed_middle = middle_msgs[-(new_limit - 2):] if len(middle_msgs) > (new_limit - 2) else middle_msgs

                if system_msg:
                    conversation_state["messages"] = [system_msg] + trimmed_middle + [user_msg]
                else:
                    conversation_state["messages"] = trimmed_middle + [user_msg]

                error_msg = AIMessage(content=f"""‚ö†Ô∏è Context window overflow detected and auto-fixed.

**Issue:** Your conversation ({requested} tokens) exceeded the model's limit ({available} tokens).

**Auto-recovery:** Reduced history from {current_msg_count} to {len(conversation_state['messages'])} messages.

**Suggestions:**
1. Start a new chat
2. `:model qwen2.5:14b` - Switch to larger model (8K tokens)
3. Keep conversations shorter with small models

**You can retry your request now.**""")
            else:
                # Can't trim further - conversation too short but still overflowing
                logger.error(f"‚ùå Cannot auto-recover: conversation already minimal ({current_msg_count} messages)")
                error_msg = AIMessage(content=f"""‚ùå Context window overflow - this model is too small for your task.

**Problem:** Even a minimal conversation exceeds this model's {available if available else '?'} token limit.

**Solutions:**
1. Start a new chat session
2. `:model qwen2.5:14b` - Switch to larger model (8K context)
3. Use a model with more capacity

This model cannot handle your current workload.""")

            conversation_state["messages"].append(error_msg)

            if METRICS_AVAILABLE:
                metrics["agent_errors"] += 1
                duration = time.time() - start_time
                metrics["agent_times"].append((time.time(), duration))

            return {"messages": conversation_state["messages"]}

        # Re-raise if not context overflow
        raise

    except Exception as e:
        if METRICS_AVAILABLE:
            metrics["agent_errors"] += 1
            duration = time.time() - start_time
            metrics["agent_times"].append((time.time(), duration))

        logger.exception(f"‚ùå Unexpected error in agent execution")
        error_msg = AIMessage(content=f"An error occurred: {str(e)}")
        conversation_state["messages"].append(error_msg)
        return {"messages": conversation_state["messages"]}