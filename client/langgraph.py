"""
LangGraph Module with Metrics Tracking AND STOP SIGNAL HANDLING
Handles LangGraph agent creation, routing, and execution with performance metrics
"""
import asyncio
import json
import logging
import operator
import time
from typing import TypedDict, Annotated, Sequence
from .stop_signal import is_stop_requested, clear_stop
from .langsearch_client import get_langsearch_client
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode

# Try to import metrics, but don't fail if not available
try:
    from metrics import metrics
    METRICS_AVAILABLE = True
except ImportError:
    try:
        from client.metrics import metrics
        METRICS_AVAILABLE = True
    except ImportError:
        METRICS_AVAILABLE = False
        # Create dummy metrics if not available
        from collections import defaultdict
        metrics = {
            "agent_runs": 0,
            "agent_errors": 0,
            "agent_times": [],
            "llm_calls": 0,
            "llm_errors": 0,
            "llm_times": [],
            "tool_calls": defaultdict(int),
            "tool_errors": defaultdict(int),
            "tool_times": defaultdict(list),
        }


class AgentState(TypedDict):
    """State that gets passed between nodes in the graph"""
    messages: Annotated[Sequence[BaseMessage], operator.add]
    tools: dict
    llm: object
    ingest_completed: bool
    stopped: bool  # NEW: Track if execution was stopped


def router(state):
    """
    Route based on what the agent decided to do
    WITH STOP SIGNAL HANDLING AND A2A LOOP PREVENTION
    """
    last_message = state["messages"][-1]

    logger = logging.getLogger("mcp_client")
    logger.debug(f"ğŸ¯ Router: Last message type = {type(last_message).__name__}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PRIORITY CHECK: Stop signal (highest priority)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if is_stop_requested():
        logger.warning(f"ğŸ›‘ Router: Stop requested - ending graph execution")
        state["stopped"] = True
        return "continue"  # Go to END

    if state.get("stopped", False):
        logger.warning(f"ğŸ›‘ Router: Execution already stopped - ending")
        return "continue"

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # A2A COMPLETION CHECK: Stop after A2A tool result (FIRST PRIORITY)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    from langchain_core.messages import ToolMessage

    # Check if last message is a ToolMessage from an A2A tool
    if isinstance(last_message, ToolMessage):
        if hasattr(last_message, 'name') and last_message.name in ["send_a2a", "discover_a2a", "send_a2a_streaming",
                                                                   "send_a2a_batch"]:
            logger.info(f"ğŸ›‘ Router: {last_message.name} result received - ending execution")
            return "continue"  # Go to END

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # If LLM made tool calls, execute them first
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if isinstance(last_message, AIMessage):
        tool_calls = getattr(last_message, "tool_calls", [])
        if tool_calls and len(tool_calls) > 0:
            logger.debug(f"ğŸ¯ Router: Found {len(tool_calls)} tool calls - routing to TOOLS")
            return "tools"

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Detect tool result messages
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if isinstance(last_message, AIMessage):
        if hasattr(last_message, "tool_call_id"):
            # This is a tool RESULT, not a tool call
            logger.info("ğŸ›‘ Router: Tool result detected - stopping")
            return "continue"

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Check if we just completed an ingest operation
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    ingest_completed = state.get("ingest_completed", False)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Check if user's ORIGINAL message requested something
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    user_message = None
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            user_message = msg
            break

    if user_message:
        content = user_message.content.lower()
        logger.debug(f"ğŸ¯ Router: Checking user's original message: {content[:100]}")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # A2A EXPLICIT ROUTING - ONLY ROUTE IF NOT ALREADY EXECUTED
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if isinstance(user_message, HumanMessage):
            if "send_a2a" in content or "discover_a2a" in content or "a2a" in content:
                # Check if we already have a ToolMessage for A2A tools
                has_a2a_result = False
                from langchain_core.messages import ToolMessage
                for msg in reversed(state["messages"]):
                    if isinstance(msg, ToolMessage) and hasattr(msg, 'name'):
                        if msg.name in ["send_a2a", "discover_a2a", "send_a2a_streaming", "send_a2a_batch"]:
                            has_a2a_result = True
                            logger.info("ğŸ›‘ Router: A2A already executed - ending")
                            break

                if not has_a2a_result:
                    logger.info("ğŸ¯ Router: Explicit A2A request detected - routing to tools")
                    return "tools"
                else:
                    return "continue"  # A2A done, end execution

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # INGEST ROUTING
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if "ingest" in content and not ingest_completed:
            # Check if user wants to stop after one batch
            if any(stop_word in content for stop_word in ["stop", "then stop", "don't continue", "don't go on"]):
                logger.info(f"ğŸ¯ Router: User requested ONE-TIME ingest - routing there")
                return "ingest"

            # Check if this is a multi-step query
            multi_step_indicators = [
                " and then ", " then ", " after that ", " next ",
                "first", "research.*analyze", "find.*summarize",
                "analyze", "create", "summary", "report"
            ]

            import re
            has_multiple_steps = any(re.search(indicator, content.lower()) for indicator in multi_step_indicators)

            if has_multiple_steps:
                logger.info(f"ğŸ¯ Router: INGEST detected with multiple steps - using MULTI-AGENT")
                return "continue"  # Let normal flow handle multi-agent
            else:
                logger.info(f"ğŸ¯ Router: User requested INGEST (simple) - routing there")
                return "ingest"  # Simple ingest, use single-agent

        elif "ingest" in content and ingest_completed:
            logger.info(f"ğŸ¯ Router: Ingest already completed - skipping to END")
            return "continue"

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # EXPLICIT RAG REQUESTS
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if any(keyword in content for keyword in
               ["using rag", "use rag", "rag tool", "with rag", "search rag", "query rag"]):
            logger.info(f"ğŸ¯ Router: User explicitly requested RAG - routing there")
            return "rag"

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # If the AI made tool calls (check again for any late additions)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if isinstance(last_message, AIMessage):
        tool_calls = getattr(last_message, "tool_calls", [])
        logger.debug(f"ğŸ¯ Router: Found {len(tool_calls)} tool calls")
        if tool_calls and len(tool_calls) > 0:
            logger.debug(f"ğŸ¯ Router: Routing to TOOLS")
            return "tools"

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Detect tool result messages (final check)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if isinstance(last_message, AIMessage):
        if hasattr(last_message, "tool_call_id"):
            logger.info("ğŸ›‘ Router: Tool result detected - stopping")
            return "continue"

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # RAG-STYLE QUESTIONS (knowledge base queries)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if isinstance(last_message, HumanMessage):
        content = last_message.content.lower()
        if not any(keyword in content for keyword in ["movie", "plex", "search", "find", "show", "media"]):
            if any(keyword in content for keyword in ["what is", "who is", "explain", "tell me about"]):
                logger.info(f"ğŸ¯ Router: Routing to RAG (knowledge query)")
                return "rag"

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # DEFAULT: Continue with normal agent completion
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    logger.debug(f"ğŸ¯ Router: Continuing to END (normal completion)")
    return "continue"

async def rag_node(state):
    """
    Search RAG and provide context to answer the question
    NOW WITH STOP SIGNAL CHECK
    """
    logger = logging.getLogger("mcp_client")

    # Check stop signal first
    if is_stop_requested():
        logger.warning("ğŸ›‘ RAG node: Stop requested - skipping RAG search")
        msg = AIMessage(content="Search cancelled by user.")
        return {
            "messages": state["messages"] + [msg],
            "llm": state.get("llm"),
            "stopped": True
        }

    # Get the user's original question (most recent HumanMessage)
    user_message = None
    for msg in reversed(state["messages"]):
        if isinstance(msg, HumanMessage):
            user_message = msg
            break

    if not user_message:
        logger.error("âŒ No user message found in RAG node")
        msg = AIMessage(content="Error: Could not find user's question.")
        return {"messages": state["messages"] + [msg], "llm": state.get("llm")}

    original_query = user_message.content

    # Extract the actual search terms from the query
    search_query = original_query.lower()
    for phrase in ["using the rag tool", "use the rag tool", "using rag", "use rag", "with rag",
                   "search rag for", "query rag for", "rag search for", "and my plex library",
                   "in my plex library", "from my plex library", "in my plex collection",
                   "from my plex collection"]:
        search_query = search_query.replace(phrase, "")

    search_query = search_query.strip().strip(",").strip()

    logger.info(f"ğŸ” RAG Node - Original query: {original_query}")
    logger.info(f"ğŸ” RAG Node - Cleaned search query: {search_query}")

    # Find the rag_search_tool
    tools_dict = state.get("tools", {})
    rag_search_tool = None

    available_tools = []
    for tool in tools_dict.values() if isinstance(tools_dict, dict) else tools_dict:
        if hasattr(tool, 'name'):
            available_tools.append(tool.name)
            if tool.name == "rag_search_tool":
                rag_search_tool = tool
                break

    logger.info(f"ğŸ” RAG Node - Available tools: {available_tools}")
    logger.info(f"ğŸ” RAG Node - Looking for 'rag_search_tool'")

    if not rag_search_tool:
        logger.error(f"âŒ RAG search tool not found! Available: {available_tools}")
        msg = AIMessage(content=f"RAG search is not available. Available tools: {', '.join(available_tools)}")
        return {"messages": state["messages"] + [msg], "llm": state.get("llm")}

    try:
        logger.info(f"ğŸ” Calling rag_search_tool with query: {search_query}")

        # Track tool call timing
        tool_start = time.time()
        result = await rag_search_tool.ainvoke({"query": search_query})
        tool_duration = time.time() - tool_start

        if METRICS_AVAILABLE:
            metrics["tool_calls"]["rag_search_tool"] += 1
            metrics["tool_times"]["rag_search_tool"].append((time.time(), tool_duration))
            logger.info(f"ğŸ“Š Tracked rag_search_tool: {tool_duration:.2f}s")

        logger.info(f"ğŸ” RAG tool result type: {type(result)}")
        logger.debug(f"ğŸ” RAG tool result (first 200 chars): {str(result)[:200]}")

        # Handle different result types
        if isinstance(result, list) and len(result) > 0:
            if hasattr(result[0], 'text'):
                logger.info("ğŸ” Detected actual TextContent object list")
                result_text = result[0].text
                try:
                    result = json.loads(result_text)
                    logger.info("âœ… Successfully parsed JSON from TextContent object")
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ JSON decode error from TextContent: {e}")
                    logger.error(f"âŒ TextContent string: {result_text[:500]}")
                    msg = AIMessage(content=f"Error parsing RAG results: {str(e)}")
                    return {"messages": state["messages"] + [msg], "llm": state.get("llm")}
        elif isinstance(result, str):
            if result.startswith("[TextContent("):
                logger.info("ğŸ” Detected TextContent string representation")

                try:
                    json_start_marker = "text='"
                    json_start_idx = result.find(json_start_marker)

                    if json_start_idx == -1:
                        raise ValueError("Could not find text=' marker")

                    json_start_idx += len(json_start_marker)

                    brace_count = 0
                    in_string = False
                    escape_next = False
                    json_end_idx = json_start_idx

                    for i in range(json_start_idx, len(result)):
                        char = result[i]

                        if escape_next:
                            escape_next = False
                            continue

                        if char == '\\':
                            escape_next = True
                            continue

                        if char == '"' and not in_string:
                            in_string = True
                        elif char == '"' and in_string:
                            in_string = False
                        elif char == '{' and not in_string:
                            brace_count += 1
                        elif char == '}' and not in_string:
                            brace_count -= 1
                            if brace_count == 0:
                                json_end_idx = i + 1
                                break

                    if json_end_idx == json_start_idx:
                        raise ValueError("Could not find end of JSON")

                    json_str = result[json_start_idx:json_end_idx]

                    import codecs
                    try:
                        json_str = codecs.decode(json_str, 'unicode_escape')
                    except Exception as decode_err:
                        logger.warning(f"âš ï¸ Codecs decode failed: {decode_err}, trying manual decode")
                        json_str = json_str.replace('\\n', '\n').replace('\\t', '\t').replace('\\r', '\r')
                        json_str = json_str.replace('\\\\', '\\').replace('\\"', '"')

                    logger.debug(f"ğŸ” Extracted JSON (first 100 chars): {json_str[:100]}")

                    result = json.loads(json_str)
                    logger.info("âœ… Successfully parsed JSON from TextContent string")

                except (ValueError, json.JSONDecodeError) as e:
                    logger.error(f"âŒ Error parsing TextContent: {e}")
                    logger.error(f"âŒ Result sample: {result[:500]}")
                    msg = AIMessage(content=f"Error parsing RAG results: {str(e)}")
                    return {"messages": state["messages"] + [msg], "llm": state.get("llm")}
            else:
                try:
                    result = json.loads(result)
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ JSON decode error: {e}")
                    logger.error(f"âŒ Result string: {result[:500]}")
                    msg = AIMessage(content=f"Error parsing RAG results: {str(e)}")
                    return {"messages": state["messages"] + [msg], "llm": state.get("llm")}

        chunks = []
        if isinstance(result, dict):
            results_list = result.get("results", [])
            chunks = [item.get("text", "") for item in results_list if isinstance(item, dict)]
            logger.info(f"âœ… Extracted {len(chunks)} chunks from RAG results")

            for i, chunk in enumerate(chunks[:3]):
                logger.debug(f"ğŸ“„ Chunk {i+1} preview: {chunk[:150]}...")

        if not chunks:
            logger.warning("âš ï¸ No chunks found in RAG results")
            msg = AIMessage(content="I couldn't find any relevant information in the knowledge base for your query.")
            return {"messages": state["messages"] + [msg], "llm": state.get("llm")}

        # Take top 3 chunks
        context = "\n\n---\n\n".join(chunks[:3])
        logger.info(f"ğŸ“„ Using top 3 chunks as context")

        # DIAGNOSTIC: Log what we're sending
        logger.debug("=" * 80)
        logger.debug("ğŸ” CONTEXT BEING SENT TO LLM:")
        logger.debug("=" * 80)
        logger.debug(context[:1500])
        if len(context) > 1500:
            logger.debug(f"... (truncated, total: {len(context)} chars)")
        logger.debug("=" * 80)

        # Create fresh conversation with ONLY the context
        augmented_messages = [
            SystemMessage(content=f"""You are answering a question about movies in a user's Plex library.

The library has already been searched. Here are the ACTUAL RESULTS:

{context}

Your job: Answer the question using ONLY the movies listed above.

CORRECT response format:
"Based on your Plex library, here are movies that match:

1. [Title from results above] ([Year]) - [Brief description from results]
2. [Title from results above] ([Year]) - [Brief description from results]

etc."

WRONG responses:
- Suggesting to use tools (the search already happened!)
- Mentioning movies not in the results above
- Saying "let's search" or "we can use"

The movies shown above ARE the search results. Just present them."""),
            user_message
        ]

        llm = state.get("llm")
        logger.debug(f"ğŸ” LLM from state: type={type(llm)}, value={llm}")

        if not llm or not hasattr(llm, 'ainvoke'):
            logger.warning("âš ï¸ LLM not provided or invalid in state, creating new instance")
            from langchain_ollama import ChatOllama
            llm = ChatOllama(model="llama3.1:8b", temperature=0)
            logger.info("ğŸ“ Created new LLM instance for RAG")

        logger.info("ğŸ§  Calling LLM with RAG context")

        # Track LLM call for RAG
        llm_start = time.time()
        response = await llm.ainvoke(augmented_messages)
        llm_duration = time.time() - llm_start

        if METRICS_AVAILABLE:
            metrics["llm_calls"] += 1
            metrics["llm_times"].append((time.time(), llm_duration))

        logger.info(f"âœ… RAG response generated: {response.content[:100]}...")

        return {"messages": state["messages"] + [response], "llm": state.get("llm")}

    except Exception as e:
        logger.error(f"âŒ Error in RAG node: {e}")
        if METRICS_AVAILABLE:
            metrics["tool_errors"]["rag_search_tool"] += 1
        msg = AIMessage(content=f"Error searching knowledge base: {str(e)}")
        return {"messages": state["messages"] + [msg], "llm": state.get("llm")}


def filter_tools_by_intent(user_message: str, all_tools: list) -> list:
    """
    Filter tools based on user intent to reduce confusion.
    Only show the LLM the tools relevant to the current request.
    """
    user_message_lower = user_message.lower()
    logger = logging.getLogger("mcp_client")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # NO TOOLS NEEDED - General knowledge questions
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # Explicit "without tools" instruction
    if "without using tools" in user_message_lower or "don't use tools" in user_message_lower:
        logger.info("ğŸ¯ Explicit NO TOOLS request - returning empty tool list")
        return []

    # Expand contractions for better matching
    expanded_message = user_message_lower
    contractions = {
        "who's": "who is",
        "what's": "what is",
        "where's": "where is",
        "when's": "when is",
        "how's": "how is",
        "that's": "that is",
        "there's": "there is",
        "it's": "it is"
    }
    for contraction, expansion in contractions.items():
        expanded_message = expanded_message.replace(contraction, expansion)

    # General knowledge questions that don't need tools
    general_knowledge_patterns = [
        "who is", "what is", "what are", "what was", "who was",
        "explain", "tell me about", "describe", "define",
        "where is", "when is", "how is"
    ]

    # Check if it's a general knowledge question WITHOUT any tool-specific keywords
    is_general_knowledge = any(pattern in expanded_message for pattern in general_knowledge_patterns)

    # Keywords that indicate tools ARE needed
    tool_keywords = [
        "my", "search", "find", "list", "show me", "get",
        "plex", "movie", "todo", "task", "note", "entry",
        "weather", "system", "code", "ingest", "rag"
    ]

    needs_tools = any(keyword in user_message_lower for keyword in tool_keywords)

    if is_general_knowledge and not needs_tools:
        logger.info("ğŸ¯ General knowledge question - no tools needed")
        return []

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # A2A TOOLS - High Priority
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # Developer override: explicit A2A tool names
    if "discover_a2a" in user_message_lower:
        logger.info("ğŸ¯ Explicit A2A override: discover_a2a")
        return [t for t in all_tools if t.name == "discover_a2a"]

    if "send_a2a_streaming" in user_message_lower:
        logger.info("ğŸ¯ Explicit A2A override: send_a2a_streaming")
        return [t for t in all_tools if t.name == "send_a2a_streaming"]

    if "send_a2a_batch" in user_message_lower:
        logger.info("ğŸ¯ Explicit A2A override: send_a2a_batch")
        return [t for t in all_tools if t.name == "send_a2a_batch"]

    if "send_a2a" in user_message_lower or "a2a" in user_message_lower:
        logger.info("ğŸ¯ Explicit A2A override: all A2A tools")
        # Return ALL A2A tools so LLM can choose
        return [t for t in all_tools if t.name in ["send_a2a", "send_a2a_streaming", "send_a2a_batch"]]

    # General A2A keywords
    a2a_keywords = [
        "send to remote",
        "ask the remote agent",
        "use a2a",
        "using a2a",
        "call the remote agent",
        "ask the other agent",
        "remote tool",
        "remote agent"
    ]

    if any(keyword in user_message_lower for keyword in a2a_keywords):
        logger.info("ğŸ¯ Detected A2A intent")
        return [t for t in all_tools if t.name in ["send_a2a", "send_a2a_streaming", "send_a2a_batch", "discover_a2a"]]

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TODO/TASK TOOLS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # To-do/task keywords - COMPREHENSIVE LIST
    todo_keywords = [
        # Adding
        "add to my todo", "add to my tasks", "remind me to", "i need to", "don't forget",
        "create a todo", "create a task", "new todo", "new task",

        # Viewing/Listing
        "todo list", "my todos", "my tasks", "task list", "what do i need to do",
        "show my todos", "list my todos", "show my tasks", "what's in my todo",
        "what's on my todo", "check my todos", "view my todos", "see my todos",
        "display todos", "display tasks", "show tasks", "list tasks",

        # Status-specific
        "incomplete todos", "non complete todos", "unfinished todos", "open todos",
        "pending todos", "active todos", "incomplete tasks", "unfinished tasks",
        "open tasks", "pending tasks", "active tasks",

        # Searching
        "find todos", "search todos", "find tasks", "search tasks",
        "todos about", "tasks about", "todos for", "tasks for",

        # Updating
        "update todo", "update task", "change todo", "modify todo",
        "mark as complete", "mark as done", "complete todo", "finish todo",

        # Deleting
        "delete todo", "remove todo", "delete task", "remove task",
        "clear todos", "clear tasks"
    ]

    if any(keyword in user_message_lower for keyword in todo_keywords):
        logger.info("ğŸ¯ Detected TODO intent")
        return [t for t in all_tools if t.name in [
            "add_todo_item", "list_todo_items", "search_todo_items",
            "update_todo_item", "delete_todo_item", "delete_all_todo_items"
        ]]

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # KNOWLEDGE BASE / NOTES TOOLS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # Note/memory keywords
    note_keywords = [
        "remember", "save this", "make a note", "write down", "store this",
        "note that", "keep track of", "record this", "jot down",
        "save note", "add note", "create note", "add entry", "list entries",
        "search entries", "knowledge base"
    ]

    if any(keyword in user_message_lower for keyword in note_keywords):
        logger.info("ğŸ¯ Detected MEMORY/NOTE intent")
        return [t for t in all_tools if t.name in [
            "add_entry", "list_entries", "get_entry", "search_entries",
            "search_by_tag", "search_semantic", "update_entry", "delete_entry"
        ]]

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # RAG SEARCH TOOLS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # RAG search keywords
    rag_search_keywords = [
        "using the rag tool", "search my notes", "what do i know about",
        "find information", "search for information", "look up in notes",
        "what did i save about", "search notes", "find in notes",
        "rag search", "search rag", "query rag"
    ]

    if any(keyword in user_message_lower for keyword in rag_search_keywords):
        logger.info("ğŸ¯ Detected RAG SEARCH intent")
        return [t for t in all_tools if t.name in [
            "rag_search_tool", "search_entries", "search_semantic", "search_by_tag"
        ]]

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PLEX INGESTION TOOLS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # PLEX INGESTION keywords
    ingest_keywords = [
        "ingest", "ingest from plex", "ingest plex", "process subtitles",
        "add to rag", "ingest items", "ingest next", "ingest from my plex",
        "process plex", "add plex to rag"
    ]

    if any(keyword in user_message_lower for keyword in ingest_keywords):
        logger.info("ğŸ¯ Detected PLEX INGEST intent")
        return [t for t in all_tools if t.name in [
            "plex_find_unprocessed", "plex_ingest_items",
            "plex_ingest_single", "plex_ingest_batch",
            "plex_get_stats", "rag_search_tool",
        ]]

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MEDIA/PLEX SEARCH TOOLS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # Media/Plex keywords
    media_keywords = [
        "find movie", "find movies", "search plex", "what movies", "show me",
        "movies about", "films about", "search for movie", "look for movie",
        "search media", "find film", "find films", "scene", "locate scene"
    ]

    if any(keyword in user_message_lower for keyword in media_keywords):
        logger.info("ğŸ¯ Detected MEDIA search intent")
        return [t for t in all_tools if t.name in [
            "semantic_media_search_text", "scene_locator_tool", "find_scene_by_title"
        ]]

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # WEATHER TOOLS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # Weather keywords
    if any(keyword in user_message_lower for keyword in ["weather", "temperature", "forecast"]):
        logger.info("ğŸ¯ Detected WEATHER intent")
        return [t for t in all_tools if t.name in [
            "get_weather_tool", "get_location_tool"
        ]]

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SYSTEM/HARDWARE TOOLS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # System/hardware keywords
    if any(keyword in user_message_lower for keyword in [
        "system", "hardware", "cpu", "gpu", "memory", "processes", "specs"
    ]):
        logger.info("ğŸ¯ Detected SYSTEM intent")
        return [t for t in all_tools if t.name in [
            "get_hardware_specs_tool", "get_system_info", "list_system_processes", "terminate_process"
        ]]

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CODE REVIEW TOOLS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # Code review keywords
    code_keywords = [
        "code", "review code", "scan directory", "search code",
        "summarize code", "debug", "fix bug", "codebase"
    ]

    if any(keyword in user_message_lower for keyword in code_keywords):
        logger.info("ğŸ¯ Detected CODE REVIEW intent")
        return [t for t in all_tools if t.name in [
            "scan_code_directory", "search_code_in_directory",
            "summarize_code_file", "summarize_code", "debug_fix"
        ]]

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TEXT PROCESSING TOOLS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # Text processing keywords
    text_keywords = [
        "summarize", "explain", "simplify", "contextualize",
        "split text", "merge summaries"
    ]

    if any(keyword in user_message_lower for keyword in text_keywords):
        logger.info("ğŸ¯ Detected TEXT PROCESSING intent")
        return [t for t in all_tools if t.name in [
            "summarize_text_tool", "summarize_direct_tool", "explain_simplified_tool",
            "concept_contextualizer_tool", "split_text_tool", "summarize_chunk_tool",
            "merge_summaries_tool"
        ]]

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # DEFAULT: Return all tools
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    logger.warning(f"ğŸ¯ No specific intent detected for: '{user_message}' - using all {len(all_tools)} tools")
    return all_tools

def create_langgraph_agent(llm_with_tools, tools):
    """Create and compile the LangGraph agent"""
    logger = logging.getLogger("mcp_client")

    # IMPORTANT: Store the base LLM (without tools) for dynamic binding
    # llm_with_tools is a RunnableBinding, we need the underlying LLM
    base_llm = llm_with_tools.bound if hasattr(llm_with_tools, 'bound') else llm_with_tools

    async def call_model(state: AgentState):
        # Check stop signal first
        if is_stop_requested():
            logger.warning("ğŸ›‘ call_model: Stop requested BEFORE calling LLM")
            empty_response = AIMessage(content="Operation cancelled by user.")
            return {
                "messages": state["messages"] + [empty_response],
                "tools": state.get("tools", {}),
                "llm": state.get("llm"),
                "ingest_completed": state.get("ingest_completed", False),
                "stopped": True
            }

        messages = state["messages"]

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Check if we're formatting tool results
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        from langchain_core.messages import ToolMessage

        last_message = messages[-1] if messages else None

        if isinstance(last_message, ToolMessage):
            logger.info("ğŸ¯ Formatting tool results")

            logger.info(f"ğŸ§  Calling LLM with {len(messages)} messages")

            start_time = time.time()
            try:
                response = await base_llm.ainvoke(messages)
                duration = time.time() - start_time

                if METRICS_AVAILABLE:
                    metrics["llm_calls"] += 1
                    metrics["llm_times"].append((time.time(), duration))

                return {
                    "messages": messages + [response],
                    "tools": state.get("tools", {}),
                    "llm": state.get("llm"),
                    "ingest_completed": state.get("ingest_completed", False),
                    "stopped": state.get("stopped", False)
                }

            except Exception as e:
                duration = time.time() - start_time
                if METRICS_AVAILABLE:
                    metrics["llm_errors"] += 1
                    metrics["llm_times"].append((time.time(), duration))
                logger.error(f"âŒ Model call failed: {e}")
                raise

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Check A2A execution
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        has_executed_a2a = False

        last_human_idx = -1
        for i in range(len(messages) - 1, -1, -1):
            if isinstance(messages[i], HumanMessage):
                last_human_idx = i
                break

        if last_human_idx >= 0:
            messages_this_turn = messages[last_human_idx + 1:]
            for msg in messages_this_turn:
                if isinstance(msg, ToolMessage) and hasattr(msg, 'name'):
                    if msg.name in ["send_a2a", "discover_a2a", "send_a2a_streaming", "send_a2a_batch"]:
                        has_executed_a2a = True
                        logger.info(f"ğŸ¯ A2A execution: {msg.name}")
                        break

        # Get user message
        user_message = None
        for msg in reversed(messages):
            if isinstance(msg, HumanMessage):
                user_message = msg.content
                break

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # MINIMAL KEYWORD ROUTING (3 patterns only)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if user_message and not has_executed_a2a:
            all_tools = list(state.get("tools", {}).values())
            user_lower = user_message.lower()

            # Pattern 1: Location queries
            if "my location" in user_lower or "what's my location" in user_lower or "where am i" in user_lower:
                logger.info("ğŸ¯ Location â†’ get_location_tool")
                filtered_tools = [t for t in all_tools if t.name == "get_location_tool"]
                llm_to_use = base_llm.bind_tools(filtered_tools if filtered_tools else all_tools)

            # Pattern 2: Weather queries (BEFORE "current" pattern)
            elif "weather" in user_lower or "temperature" in user_lower or "forecast" in user_lower:
                logger.info("ğŸ¯ Weather â†’ location + weather tools")
                filtered_tools = [t for t in all_tools if t.name in ["get_location_tool", "get_weather_tool"]]
                llm_to_use = base_llm.bind_tools(filtered_tools if filtered_tools else all_tools)

            # Pattern 3: Time queries
            elif "what time" in user_lower or "current time" in user_lower:
                logger.info("ğŸ¯ Time â†’ get_time_tool")
                filtered_tools = [t for t in all_tools if t.name == "get_time_tool"]
                llm_to_use = base_llm.bind_tools(filtered_tools if filtered_tools else all_tools)

            # Pattern 4: Todo/task management
            elif any(kw in user_lower for kw in ["todo", "task", "remind me"]) and not "find" in user_lower:
                logger.info("ğŸ¯ Todo â†’ todo tools")
                filtered_tools = [t for t in all_tools if "todo" in t.name.lower()]
                llm_to_use = base_llm.bind_tools(filtered_tools if filtered_tools else all_tools)

            # Pattern 5: Plex library searches
            elif ("find" in user_lower or "search" in user_lower) and (
                    "plex" in user_lower or "library" in user_lower or "my library" in user_lower):
                logger.info("ğŸ¯ Plex library â†’ RAG + scene tools")
                filtered_tools = [t for t in all_tools if
                                  t.name in ["rag_search_tool", "semantic_media_search_text", "scene_locator_tool",
                                             "find_scene_by_title"]]
                llm_to_use = base_llm.bind_tools(filtered_tools if filtered_tools else all_tools)

            # Pattern 6: System info queries
            elif any(kw in user_lower for kw in ["system info", "hardware", "cpu", "gpu", "ram", "specs", "processes"]):
                logger.info("ğŸ¯ System â†’ system info tools")
                filtered_tools = [t for t in all_tools if
                                  t.name in ["get_hardware_specs_tool", "get_system_info", "list_system_processes"]]
                llm_to_use = base_llm.bind_tools(filtered_tools if filtered_tools else all_tools)

            # Pattern 7: Code-related queries
            elif any(kw in user_lower for kw in ["code", "scan code", "debug", "review code", "summarize code"]):
                logger.info("ğŸ¯ Code â†’ code tools")
                filtered_tools = [t for t in all_tools if
                                  t.name in ["summarize_code_file", "search_code_in_directory", "scan_code_directory",
                                             "summarize_code", "debug_fix"]]
                llm_to_use = base_llm.bind_tools(filtered_tools if filtered_tools else all_tools)

            # Pattern 8: Text summarization
            elif any(kw in user_lower for kw in ["summarize", "summary", "explain"]) and not "code" in user_lower:
                logger.info("ğŸ¯ Text â†’ text/summarization tools")
                filtered_tools = [t for t in all_tools if
                                  t.name in ["summarize_text_tool", "summarize_direct_tool", "explain_simplified_tool",
                                             "concept_contextualizer_tool"]]
                llm_to_use = base_llm.bind_tools(filtered_tools if filtered_tools else all_tools)

            # Pattern 9: Plex ingestion
            elif "ingest" in user_lower:
                logger.info("ğŸ¯ Ingest â†’ plex ingest tools")
                filtered_tools = [t for t in all_tools if
                                  "ingest" in t.name.lower() or t.name in ["plex_find_unprocessed", "plex_get_stats",
                                                                           "rag_status_tool", "rag_diagnose_tool"]]
                llm_to_use = base_llm.bind_tools(filtered_tools if filtered_tools else all_tools)

            # Pattern 10: A2A/remote agent queries
            elif "a2a" in user_lower or "remote" in user_lower or "discover" in user_lower:
                logger.info("ğŸ¯ A2A â†’ a2a tools")
                filtered_tools = [t for t in all_tools if "a2a" in t.name.lower()]
                llm_to_use = base_llm.bind_tools(filtered_tools if filtered_tools else all_tools)

            # Pattern 11: Current events / general knowledge (AFTER all specific patterns)
            elif "current" in user_lower or "who is" in user_lower or "what is" in user_lower:
                logger.info("ğŸ¯ Current/general knowledge â†’ LangSearch")

                langsearch = get_langsearch_client()

                if langsearch.is_available():
                    search_result = await langsearch.search(user_message)

                    if search_result["success"]:
                        logger.info("âœ… LangSearch successful")
                        search_context = search_result["results"]

                        augmented_messages = []
                        for msg in messages:
                            if isinstance(msg, SystemMessage):
                                augmented_messages.append(SystemMessage(content=f"""You are a helpful AI assistant.

        WEB SEARCH RESULTS:
        {search_context}

        Use these results to answer accurately."""))
                            else:
                                augmented_messages.append(msg)

                        messages = augmented_messages
                        llm_to_use = base_llm
                    else:
                        logger.warning(f"âš ï¸ LangSearch failed - using base LLM")
                        llm_to_use = base_llm
                else:
                    logger.warning("âš ï¸ LangSearch not available - using base LLM")
                    llm_to_use = base_llm

            # Everything else: Use all tools
            else:
                logger.info(f"ğŸ¯ General query â†’ all {len(all_tools)} tools")
                llm_to_use = base_llm.bind_tools(all_tools)

        elif has_executed_a2a:
            logger.info("ğŸ¯ A2A executed - no tools")
            llm_to_use = base_llm
        else:
            llm_to_use = llm_with_tools

        logger.info(f"ğŸ§  Calling LLM with {len(messages)} messages")

        start_time = time.time()
        try:
            llm_task = asyncio.create_task(llm_to_use.ainvoke(messages))

            timeout_seconds = 120
            elapsed = 0

            while not llm_task.done():
                if is_stop_requested():
                    logger.warning("ğŸ›‘ Stop during LLM call")
                    llm_task.cancel()
                    try:
                        await llm_task
                    except asyncio.CancelledError:
                        pass

                    empty_response = AIMessage(content="ğŸ›‘ Operation stopped.")
                    return {
                        "messages": state["messages"] + [empty_response],
                        "tools": state.get("tools", {}),
                        "llm": state.get("llm"),
                        "ingest_completed": state.get("ingest_completed", False),
                        "stopped": True
                    }

                if elapsed >= timeout_seconds:
                    logger.error(f"âŒ LLM timeout")
                    llm_task.cancel()
                    try:
                        await llm_task
                    except asyncio.CancelledError:
                        pass

                    timeout_response = AIMessage(content="â±ï¸ Request timeout.")
                    return {
                        "messages": state["messages"] + [timeout_response],
                        "tools": state.get("tools", {}),
                        "llm": state.get("llm"),
                        "ingest_completed": state.get("ingest_completed", False),
                        "stopped": True
                    }

                await asyncio.sleep(0.05)
                elapsed += 0.05

            response = await llm_task
            duration = time.time() - start_time

            if METRICS_AVAILABLE:
                metrics["llm_calls"] += 1
                metrics["llm_times"].append((time.time(), duration))

            # Handle empty responses
            has_tool_calls = hasattr(response, 'tool_calls') and response.tool_calls
            has_content = hasattr(response, 'content') and response.content and response.content.strip()

            if not has_tool_calls and not has_content:
                logger.warning("âš ï¸ Empty response - retrying without tools")
                retry_messages = messages + [HumanMessage(content="Please provide a direct answer.")]
                retry_response = await base_llm.ainvoke(retry_messages)

                if retry_response.content and retry_response.content.strip():
                    response = retry_response
                else:
                    response = AIMessage(
                        content="I'm having trouble generating a response. Please rephrase your question.")

            return {
                "messages": messages + [response],
                "tools": state.get("tools", {}),
                "llm": state.get("llm"),
                "ingest_completed": state.get("ingest_completed", False),
                "stopped": state.get("stopped", False)
            }

        except Exception as e:
            duration = time.time() - start_time
            if METRICS_AVAILABLE:
                metrics["llm_errors"] += 1
                metrics["llm_times"].append((time.time(), duration))
            logger.error(f"âŒ Model call failed: {e}")
            raise

    async def ingest_node(state: AgentState):
        # Check stop signal first
        if is_stop_requested():
            logger.warning("ğŸ›‘ ingest_node: Stop requested - skipping ingestion")
            msg = AIMessage(content="Ingestion cancelled by user.")
            return {
                "messages": state["messages"] + [msg],
                "tools": state.get("tools", {}),
                "llm": state.get("llm"),
                "ingest_completed": True,
                "stopped": True
            }

        tools_dict = state.get("tools", {})
        ingest_tool = None

        for tool in tools_dict.values() if isinstance(tools_dict, dict) else tools_dict:
            if hasattr(tool, 'name') and tool.name == "plex_ingest_batch":
                ingest_tool = tool
                break

        if not ingest_tool:
            msg = AIMessage(content="Ingestion tool not available.")
            return {
                "messages": state["messages"] + [msg],
                "tools": state.get("tools", {}),
                "llm": state.get("llm"),
                "ingest_completed": True,
                "stopped": False
            }

        try:
            logger.info("ğŸ“¥ Starting ingest operation...")
            limit = 5
            messages = state["messages"]

            for msg in reversed(messages):
                if isinstance(msg, AIMessage) and hasattr(msg, 'tool_calls') and msg.tool_calls:
                    for tool_call in msg.tool_calls:
                        if tool_call.get('name') == 'plex_ingest_batch':
                            args = tool_call.get('args', {})
                            limit = args.get('limit', 5)
                            logger.info(f"ğŸ“¥ Using limit={limit} from LLM tool call")
                            break
                    break

            logger.info(f"ğŸ“¥ Starting ingest operation with limit={limit}...")
            result = await ingest_tool.ainvoke({"limit": limit})

            logger.debug(f"ğŸ” Raw result type: {type(result)}")
            logger.debug(f"ğŸ” Raw result: {result}")

            if isinstance(result, list) and len(result) > 0:
                if hasattr(result[0], 'text'):
                    logger.info("ğŸ” Detected TextContent object in list")
                    result = result[0].text
                    logger.debug(f"ğŸ” Extracted text from object, length: {len(result)}")

            if isinstance(result, str) and result.startswith('[TextContent('):
                logger.info("ğŸ” Detected TextContent string, extracting...")
                import re

                start_marker = "text='"
                start_idx = result.find(start_marker)

                if start_idx != -1:
                    start_idx += len(start_marker)

                    end_markers = ["', annotations=", "', type="]
                    end_idx = -1

                    for marker in end_markers:
                        idx = result.find(marker, start_idx)
                        if idx != -1:
                            if end_idx == -1 or idx < end_idx:
                                end_idx = idx

                    if end_idx != -1:
                        json_str = result[start_idx:end_idx]

                        import codecs
                        try:
                            json_str = codecs.decode(json_str, 'unicode_escape')
                        except Exception as decode_err:
                            logger.warning(f"âš ï¸ Codecs decode failed: {decode_err}, trying manual decode")
                            json_str = json_str.replace('\\n', '\n').replace('\\t', '\t')
                            json_str = json_str.replace('\\\\', '\\').replace("\\'", "'").replace('\\"', '"')

                        result = json_str
                        logger.debug(f"ğŸ” Extracted text, length: {len(result)}")

            if isinstance(result, str):
                try:
                    result = json.loads(result)
                    logger.info(f"âœ… Successfully parsed JSON result")
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ JSON decode error: {e}")
                    msg = AIMessage(
                        content=f"Error: Could not parse ingestion result. Check logs for details.")
                    return {
                        "messages": state["messages"] + [msg],
                        "tools": state.get("tools", {}),
                        "llm": state.get("llm"),
                        "ingest_completed": True,
                        "stopped": False
                    }

            # Check if ingestion was stopped
            was_stopped = result.get('stopped', False) if isinstance(result, dict) else False

            if isinstance(result, dict) and "error" in result:
                msg = AIMessage(content=f"Ingestion error: {result['error']}")
            elif was_stopped:
                # Ingestion was stopped
                stop_reason = result.get('stop_reason', 'Stopped by user')
                items_processed = result.get('items_processed', 0)
                msg = AIMessage(
                    content=f"ğŸ›‘ **Ingestion stopped:** {stop_reason}\n\n"
                            f"Items processed before stop: {items_processed}"
                )
            else:
                ingested = result.get('ingested', []) if isinstance(result, dict) else []
                remaining = result.get('remaining', 0) if isinstance(result, dict) else 0
                total_ingested = result.get('total_ingested', 0) if isinstance(result, dict) else 0

                if ingested:
                    items_list = "\n".join(f"{i + 1}. {item}" for i, item in enumerate(ingested))

                    msg = AIMessage(
                        content=f"âœ… **Successfully ingested {len(ingested)} items:**\n\n{items_list}\n\n"
                                f"ğŸ“Š **Total items in RAG:** {total_ingested}\n"
                                f"ğŸ“Š **Remaining to ingest:** {remaining}\n\n"
                                f"Ingestion complete. You can now search this content using the RAG tool."
                    )
                else:
                    msg = AIMessage(
                        content=f"âœ… All items already ingested.\n\nğŸ“Š **Total items in RAG:** {total_ingested}"
                    )

            logger.info("âœ… Ingest operation completed")

        except Exception as e:
            logger.error(f"âŒ Error in ingest_node: {e}")
            import traceback
            traceback.print_exc()
            msg = AIMessage(content=f"Ingestion failed: {str(e)}")
            was_stopped = False

        return {
            "messages": state["messages"] + [msg],
            "tools": state.get("tools", {}),
            "llm": state.get("llm"),
            "ingest_completed": True,
            "stopped": was_stopped
        }

    workflow = StateGraph(AgentState)

    workflow.add_node("agent", call_model)

    async def call_tools_with_stop_check(state: AgentState):
        """
        Custom tool executor that checks stop signal before AND during execution
        """
        logger = logging.getLogger("mcp_client")

        # Check stop BEFORE executing tools
        if is_stop_requested():
            logger.warning("ğŸ›‘ call_tools: Stop requested - skipping tool execution")
            empty_response = AIMessage(content="Tool execution cancelled by user.")
            return {
                "messages": state["messages"] + [empty_response],
                "tools": state.get("tools", {}),
                "llm": state.get("llm"),
                "ingest_completed": state.get("ingest_completed", False),
                "stopped": True
            }

        # Use the standard ToolNode for actual execution
        from langchain_core.messages import ToolMessage

        last_message = state["messages"][-1]
        tool_calls = getattr(last_message, "tool_calls", [])

        if not tool_calls:
            logger.warning("âš ï¸ call_tools: No tool calls found")
            return state

        tool_messages = []

        for tool_call in tool_calls:
            # Check stop BEFORE EACH tool execution
            if is_stop_requested():
                logger.warning(f"ğŸ›‘ call_tools: Stop requested - halting remaining tool calls")
                error_msg = ToolMessage(
                    content="Tool execution stopped by user",
                    tool_call_id=tool_call.get("id", "stopped"),
                    name=tool_call.get("name", "unknown")
                )
                tool_messages.append(error_msg)
                break

            tool_name = tool_call.get("name")
            tool_args = tool_call.get("args", {})
            tool_id = tool_call.get("id")

            logger.info(f"ğŸ”§ Executing tool: {tool_name} with args: {tool_args}")

            # Get the tool
            tools_dict = state.get("tools", {})
            tool = tools_dict.get(tool_name)

            if not tool:
                logger.error(f"âŒ Tool '{tool_name}' not found")
                error_msg = ToolMessage(
                    content=f"Error: Tool '{tool_name}' not found",
                    tool_call_id=tool_id,
                    name=tool_name
                )
                tool_messages.append(error_msg)
                continue

            try:
                # Execute the tool
                tool_start = time.time()
                result = await tool.ainvoke(tool_args)
                tool_duration = time.time() - tool_start

                # Track metrics
                if METRICS_AVAILABLE:
                    metrics["tool_calls"][tool_name] += 1
                    metrics["tool_times"][tool_name].append((time.time(), tool_duration))

                # Handle result
                if isinstance(result, list) and len(result) > 0:
                    if hasattr(result[0], 'text'):
                        result = result[0].text

                result_msg = ToolMessage(
                    content=str(result),
                    tool_call_id=tool_id,
                    name=tool_name
                )
                tool_messages.append(result_msg)

                logger.info(f"âœ… Tool {tool_name} completed in {tool_duration:.2f}s")

            except Exception as e:
                logger.error(f"âŒ Tool {tool_name} failed: {e}")

                if METRICS_AVAILABLE:
                    metrics["tool_errors"][tool_name] += 1

                error_msg = ToolMessage(
                    content=f"Error: {str(e)}",
                    tool_call_id=tool_id,
                    name=tool_name
                )
                tool_messages.append(error_msg)

        # Check if we stopped during execution
        stopped = is_stop_requested()

        return {
            "messages": state["messages"] + tool_messages,
            "tools": state.get("tools", {}),
            "llm": state.get("llm"),
            "ingest_completed": state.get("ingest_completed", False),
            "stopped": stopped
        }

    workflow.add_node("tools", call_tools_with_stop_check)
    workflow.add_node("rag", rag_node)
    workflow.add_node("ingest", ingest_node)

    workflow.set_entry_point("agent")

    workflow.add_conditional_edges(
        "agent",
        router,
        {
            "tools": "tools",
            "rag": "rag",
            "ingest": "ingest",
            "continue": END
        }
    )

    workflow.add_edge("tools", "agent")
    workflow.add_edge("ingest", END)
    workflow.add_edge("rag", END)

    app = workflow.compile()
    logger.info("âœ… LangGraph agent compiled successfully")

    return app


async def run_agent(agent, conversation_state, user_message, logger, tools, system_prompt, llm=None, max_history=20):
    """Execute the agent with the given user message and track metrics"""

    start_time = time.time()

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CLEAR STOP SIGNAL AT START OF NEW REQUEST
    # This ensures old stop requests don't block new requests
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    clear_stop()
    logger.info("âœ… Stop signal cleared for new request")

    try:
        if METRICS_AVAILABLE:
            metrics["agent_runs"] += 1

        conversation_state["loop_count"] += 1

        if conversation_state["loop_count"] >= 5:
            logger.error("âš ï¸ Loop detected â€” stopping early after 5 iterations.")
            if METRICS_AVAILABLE:
                metrics["agent_errors"] += 1
                duration = time.time() - start_time
                metrics["agent_times"].append((time.time(), duration))

            error_msg = AIMessage(
                content=(
                    "I detected that this request was causing repeated reasoning loops. "
                    "I'm stopping early to avoid getting stuck. "
                    "Try rephrasing your request or simplifying what you're asking for."
                )
            )
            conversation_state["messages"].append(error_msg)
            conversation_state["loop_count"] = 0
            return {"messages": conversation_state["messages"]}

        # Initialize with system message if needed
        if not conversation_state["messages"]:
            conversation_state["messages"].append(
                SystemMessage(content=system_prompt)
            )

        # Add the new user message
        conversation_state["messages"].append(
            HumanMessage(content=user_message)
        )

        # Trim history BEFORE invoking agent
        conversation_state["messages"] = conversation_state["messages"][-max_history:]

        # Ensure system message is at the start after trimming
        if not isinstance(conversation_state["messages"][0], SystemMessage):
            conversation_state["messages"].insert(0, SystemMessage(content=system_prompt))

        logger.info(f"ğŸ§  Starting agent with {len(conversation_state['messages'])} messages")

        tool_registry = {tool.name: tool for tool in tools}

        # Invoke the agent
        # NOTE: ainvoke is atomic - can't check stop mid-execution
        # However, individual nodes (router, call_model, ingest_node, rag_node) DO check stop
        result = await agent.ainvoke({
            "messages": conversation_state["messages"],
            "tools": tool_registry,
            "llm": llm,
            "ingest_completed": False,
            "stopped": False
        })

        new_messages = result["messages"][len(conversation_state["messages"]):]
        logger.info(f"ğŸ“¨ Agent added {len(new_messages)} new messages")
        conversation_state["messages"].extend(new_messages)

        # Check if execution was stopped
        was_stopped = result.get("stopped", False)
        if was_stopped:
            logger.warning("ğŸ›‘ Agent execution was stopped")

        # Track tool calls from AIMessages with tool_calls
        if METRICS_AVAILABLE:
            from langchain_core.messages import ToolMessage
            tool_calls_seen = set()

            for msg in new_messages:
                # Track from ToolMessage to avoid double counting
                if isinstance(msg, ToolMessage):
                    tool_name = getattr(msg, 'name', None)
                    tool_id = getattr(msg, 'tool_call_id', None)

                    if tool_name and tool_id and tool_id not in tool_calls_seen:
                        tool_calls_seen.add(tool_id)
                        metrics["tool_calls"][tool_name] += 1
                        logger.debug(f"ğŸ“Š Tracked tool: {tool_name}")

        # Reset loop count
        conversation_state["loop_count"] = 0

        # Track successful agent run
        if METRICS_AVAILABLE:
            duration = time.time() - start_time
            metrics["agent_times"].append((time.time(), duration))
            logger.info(f"âœ… Agent run completed in {duration:.2f}s (stopped={was_stopped})")

        # Debug: Log final state
        logger.debug(f"ğŸ“¨ Final conversation has {len(conversation_state['messages'])} messages")
        for i, msg in enumerate(conversation_state['messages'][-5:]):
            msg_type = type(msg).__name__
            content_preview = msg.content[:100] if hasattr(msg, 'content') else str(msg)[:100]
            logger.debug(f"  [-{5 - i}] {msg_type}: {content_preview}")

        return {"messages": conversation_state["messages"]}

    except Exception as e:
        if METRICS_AVAILABLE:
            metrics["agent_errors"] += 1
            duration = time.time() - start_time
            metrics["agent_times"].append((time.time(), duration))

        if "GraphRecursionError" in str(e):
            logger.error("âŒ Recursion limit reached â€” stopping agent loop safely.")
            error_msg = AIMessage(
                content=(
                    "I ran into a recursion limit while processing your request. "
                    "This usually means the model kept looping instead of producing a final answer. "
                    "Try rephrasing your request or simplifying what you're asking for."
                )
            )
            conversation_state["messages"].append(error_msg)
            return {"messages": conversation_state["messages"]}

        logger.exception(f"âŒ Unexpected error in agent execution")
        error_text = getattr(e, "args", [str(e)])[0]
        error_msg = AIMessage(
            content=f"An error occurred while running the agent:\n\n{error_text}"
        )
        conversation_state["messages"].append(error_msg)
        return {"messages": conversation_state["messages"]}